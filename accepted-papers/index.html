<!DOCTYPE html>
<!--
    Type on Strap jekyll theme v2.0.0
    Copyright 2016-2019 Sylhare
    Theme free for personal and commercial use under the MIT license
    https://github.com/sylhare/Type-on-Strap/blob/master/LICENSE
-->
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!-- Main JS (navbar.js, katex_init.js and masonry_init.js)-->
    <script defer=true src="/assets/js/main.min.js"></script>
    
    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!--Favicon-->
    <link rel="shortcut icon" href="" type="image/x-icon">

    <!-- Canonical -->
    <link rel="canonical" href="https://larel-ws.github.io/accepted-papers/">

    <!-- RSS -->
    <link rel="alternate" type="application/atom+xml" title="LaReL 2020" href="https://larel-ws.github.io/feed.xml"/>
    
    <!-- CSS only -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">

    <!-- JS, Popper.js, and jQuery -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>

    <!-- KaTeX 0.8.3 -->
    <!-- if you have any issue check https://github.com/KaTeX/KaTeX -->
    
    <script src="/assets/js/vendor/katex.min.js"></script>
    

    <!-- Google Analytics -->
    
    <!-- End Google Analytics -->

    <!-- seo tags -->
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Accepted Papers | LaReL 2020</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Accepted Papers" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Language in Reinforcement Learning Workshop" />
<meta property="og:description" content="Language in Reinforcement Learning Workshop" />
<link rel="canonical" href="https://larel-ws.github.io/accepted-papers/" />
<meta property="og:url" content="https://larel-ws.github.io/accepted-papers/" />
<meta property="og:site_name" content="LaReL 2020" />
<script type="application/ld+json">
{"headline":"Accepted Papers","description":"Language in Reinforcement Learning Workshop","@type":"WebPage","url":"https://larel-ws.github.io/accepted-papers/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <!-- Manual seo tags -->
    <!--
    <title>Accepted Papers | LaReL 2020</title>
    <meta name="description" content="Language in Reinforcement Learning Workshop">
    -->
</head>

  <body>
    <header class="site-header">

    <!-- Logo and title -->
	<div class="branding">
        

		<h1 class="site-title">
			<a aria-label="LaReL 2020" href="/">
        LaReL 2020
      </a>
		</h1>
	</div>

    <!-- Toggle menu -->
    <nav class="clear">
    <a aria-label="pull" id="pull" class="toggle" href="#">
    <i class="fa fa-bars fa-lg"></i>
    </a>

    <!-- Menu -->
    <ul class="hide">
        

        
            
            
        
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="Organising Committee" title="Organising Committee" href="/organisers/">
                     Organising Committee 
                </a>
            </li>
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="Accepted Papers" title="Accepted Papers" href="/accepted-papers/">
                     Accepted Papers 
                </a>
            </li>
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="Program" title="Program" href="/program/">
                     Program 
                </a>
            </li>
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="Call for Papers" title="Call for Papers" href="/submission/">
                     Call for Papers 
                </a>
            </li>
            
            
        
            
            
        
            
            
        

        
    </ul>

	</nav>
</header>

    <div class="content">
      <article >
  <header id="main" style="background-image: url('')">
  </header>
  <section class="post-content">
  
	<h1 id="accepted-papers">Accepted Papers</h1>

<div class="card mb-3">
  <div class="card-header">
    PixL2R: Guiding Reinforcement Learning using Natural Language by Mapping Pixels to Rewards
  </div>
  <div class="card-body">
    <p class="card-text">
      <p>Authors: Prasoon Goyal, Scott Niekum, Ray Mooney</p>
      <p>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=G47XFDzew2" role="button">
          OpenReview
        </a>
        -->
        <a class="btn btn-primary" href="/assets/pdfs/pixl2r_guiding_reinforcement_learning_using_natural_language_by_mapping_pixels_to_rewards.pdf" role="button">
          Paper
        </a>
        <a class="btn btn-primary" data-toggle="collapse" href="#collapseVideo-ZPPr0i52JJU" role="button" aria-expanded="false" aria-controls="collapseVideo-ZPPr0i52JJU">
          Video
        </a>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=G47XFDzew2" role="button">
          Slides
        </a>
        -->
      </p>
      <p>
      Reinforcement learning (RL), particularly in sparse reward settings, often requires prohibitively large numbers of interactions with the environment, thereby limiting its applicability to complex problems. To address this, several prior approaches have used natural language to guide the agent's exploration. However, these approaches typically operate on structured representations of the environment, and/or assume some structure in the natural language commands. In this work, we propose a model that directly maps pixels to rewards, given a free-form natural language description of the task, which can then be used for policy training. Our experiments on the Meta-World robot manipulation domain show that  language-based rewards significantly improve learning. Further, we analyze the resulting framework using multiple ablation experiments to better understand the nature of these improvements.
      </p>

      <div class="embed-responsive embed-responsive-16by9 collapse" id="collapseVideo-ZPPr0i52JJU">
        <p>
          <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/ZPPr0i52JJU" allowfullscreen=""></iframe>
        </p>
      </div>
    </p>
  </div>
</div>

<div class="card mb-3">
  <div class="card-header">
    Language-Conditioned Goal Generation: a New Approach to Language Grounding in RL
  </div>
  <div class="card-body">
    <p class="card-text">
      <p>Authors: Cédric Colas, Ahmed Akakzia, Pierre-Yves Oudeyer, Mohamed Chetouani, Olivier Sigaud</p>
      <p>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=OeLMp3kWT8y" role="button">
          OpenReview
        </a>
        -->
        <a class="btn btn-primary" href="/assets/pdfs/language_conditioned_goal_generation_a_new_approach_to_language_grounding_in_rl.pdf" role="button">
          Paper
        </a>
        <a class="btn btn-primary" data-toggle="collapse" href="#collapseVideo-fTLA_a3cITs" role="button" aria-expanded="false" aria-controls="collapseVideo-fTLA_a3cITs">
          Video
        </a>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=OeLMp3kWT8y" role="button">
          Slides
        </a>
        -->
      </p>
      <p>
      In the real world, linguistic agents are also embodied agents: they perceive and act in the physical world. The notion of Language Grounding questions the interactions between language and embodiment: how do learning agents connect or ground linguistic representations to the physical world ? This question has recently been approached by the Reinforcement Learning community under the framework of instruction-following agents. In these agents, behavioral policies or reward functions are conditioned on the embedding of an instruction expressed in natural language. This paper proposes another approach: using language to condition goal generators. Given any goal-conditioned policy, one could train a language-conditioned goal generator to generate language-agnostic goals for the agent. This method allows to decouple sensorimotor learning from language acquisition and enable agents to demonstrate a diversity of behaviors for any given instruction. We propose a particular instantiation of this approach and demonstrate its benefits.
      </p>

      <div class="embed-responsive embed-responsive-16by9 collapse" id="collapseVideo-fTLA_a3cITs">
        <p>
          <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/fTLA_a3cITs" allowfullscreen=""></iframe>
        </p>
      </div>
    </p>
  </div>
</div>

<div class="card mb-3">
  <div class="card-header">
    Language-Goal Imagination to Foster Creative Exploration in Deep RL
  </div>
  <div class="card-body">
    <p class="card-text">
      <p>Authors: Tristan Karch, Nicolas Lair, Cédric Colas, Jean-Michel Dussoux, Clément Moulin-Frier, Peter Ford Dominey, Pierre-Yves Oudeyer</p>
      <p>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=l3923_BJIAN" role="button">
          OpenReview
        </a>
        -->
        <a class="btn btn-primary" href="/assets/pdfs/language_goal_imagination_to_foster_creative_exploration_in_deep_rl.pdf" role="button">
          Paper
        </a>
        <a class="btn btn-primary" data-toggle="collapse" href="#collapseVideo-0RHx4T-wpa0" role="button" aria-expanded="false" aria-controls="collapseVideo-0RHx4T-wpa0">
          Video
        </a>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=l3923_BJIAN" role="button">
          Slides
        </a>
        -->
      </p>
      <p>
      Developmental machine learning studies how artificial agents can model the way children learn open-ended repertoires of skills. Children are known to use language and its compositionality as a tool to imagine descriptions of outcomes they never experienced before and target them as goals during play. We introduce IMAGINE, an intrinsically motivated deep RL architecture that models this ability. Such imaginative agents, like children, benefit from the guidance of a social peer who provides language descriptions. To take advantage of goal imagination, agents must be able to leverage these descriptions to interpret their imagined goals. This generalization is made possible by modularity: a decomposition between learned goal-achievement reward function and policy relying on deep sets, gated attention and object-centered representations. We introduce the Playground environment and study how this form of goal imagination improves generalization and exploration over agents lacking this capacity.
      </p>

      <div class="embed-responsive embed-responsive-16by9 collapse" id="collapseVideo-0RHx4T-wpa0">
        <p>
          <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/0RHx4T-wpa0" allowfullscreen=""></iframe>
        </p>
      </div>
    </p>
  </div>
</div>

<div class="card mb-3">
  <div class="card-header">
    Extended Abstract: Improving Vision-and-Language Navigation with Image-Text Pairs from the Web
  </div>
  <div class="card-body">
    <p class="card-text">
      <p>Authors: Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, Dhruv Batra</p>
      <p>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=sS-4cRL0yi" role="button">
          OpenReview
        </a>
        -->
        <a class="btn btn-primary" href="/assets/pdfs/improving_vision_and_language_navigation_with_image_text_pairs_from_the_web.pdf" role="button">
          Paper
        </a>
        <a class="btn btn-primary" data-toggle="collapse" href="#collapseVideo-fw2XCVZjFG0" role="button" aria-expanded="false" aria-controls="collapseVideo-fw2XCVZjFG0">
          Video
        </a>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=sS-4cRL0yi" role="button">
          Slides
        </a>
        -->
      </p>
      <p>
      We ask the following question -- can we leverage abundant 'disembodied' web-scraped vision-and-language corpora (e.g. Conceptual Captions (Sharma et al., 2018)) to learn visual groundings (what do 'stairs' look like?) that improve performance on a relatively data-starved embodied perception task (Vision-and-Language Navigation)? Specifically, we develop VLN-BERT, a visiolinguistic transformer model that scores the compatibility between an instruction ('...stop near the sofa') and a sequence of panoramic images. We demonstrate that pretraining VLN-BERT on image-text pairs from the web significantly improves performance on VLN -- outperforming the prior state-of-the-art in the fully-observed setting by 4 absolute percentage points on success rate. Ablations of our pretraining curriculum show each stage to be impactful -- with their combination resulting in further synergistic effects.
      </p>

      <div class="embed-responsive embed-responsive-16by9 collapse" id="collapseVideo-fw2XCVZjFG0">
        <p>
          <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/fw2XCVZjFG0" allowfullscreen=""></iframe>
        </p>
      </div>
    </p>
  </div>
</div>

<div class="card mb-3">
  <div class="card-header">
    On the Relationship Between Structure in Natural Language and Models of Sequential Decision Processes
  </div>
  <div class="card-body">
    <p class="card-text">
      <p>Authors: Roma Patel, Rafael Rodriguez-Sanchez, George Konidaris</p>
      <p>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=-KDnP4X1-0_" role="button">
          OpenReview
        </a>
        -->
        <a class="btn btn-primary" href="/assets/pdfs/on_the_relationship_between_structure_in_natural_language_and_models_of_sequential_decision_processes.pdf" role="button">
          Paper
        </a>
        <a class="btn btn-primary" data-toggle="collapse" href="#collapseVideo-a3JJo_cvzpE" role="button" aria-expanded="false" aria-controls="collapseVideo-a3JJo_cvzpE">
          Video
        </a>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=-KDnP4X1-0_" role="button">
          Slides
        </a>
        -->
      </p>
      <p>
      Human language is distinguished by powerful semantics, rich structure, and incredible flexibility. It enables us to communicate with each other, thereby affecting the decisions we make and actions we take. While Artificial Intelligence (AI) has made great advances both in sequential decision-making  using Markov Decision Processes (MDPs) and in Natural Language Processing (NLP), the potential of language to inform  sequential decision-making  is still unrealized. We explore how the different functional elements of natural language---such as verbs, nouns and adjectives---relate to decision process formalisms of varying complexity and structure. We attempt to determine which elements of language can be usefully grounded to a particular class of decision process and how partial observability changes the usability of language information. Our work show that  more complex, structured models can capture linguistic concepts that simple MDPs cannot. We argue that the rich structure of natural language indicates that reinforcement learning should focus on richer, more highly structured models of decision-making.
      </p>

      <div class="embed-responsive embed-responsive-16by9 collapse" id="collapseVideo-a3JJo_cvzpE">
        <p>
          <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/a3JJo_cvzpE" allowfullscreen=""></iframe>
        </p>
      </div>
    </p>
  </div>
</div>

<div class="card mb-3">
  <div class="card-header">
    An Overview of Natural Language State Representation for Reinforcement Learning
  </div>
  <div class="card-body">
    <p class="card-text">
      <p>Authors: Brielen Madureira, David Schlangen</p>
      <p>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=xoJWcw1G38g" role="button">
          OpenReview
        </a>
        -->
        <a class="btn btn-primary" href="/assets/pdfs/an_overview_of_natural_language_state_representation_for_reinforcement_learning.pdf" role="button">
          Paper
        </a>
        <a class="btn btn-primary" data-toggle="collapse" href="#collapseVideo-2zcD2zylw8U" role="button" aria-expanded="false" aria-controls="collapseVideo-2zcD2zylw8U">
          Video
        </a>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=xoJWcw1G38g" role="button">
          Slides
        </a>
        -->
      </p>
      <p>
      A suitable state representation is a fundamental part of the learning process in Reinforcement Learning. In various tasks, the state can either be described by natural language or be natural language itself. This survey outlines the strategies used in the literature to build natural language state representations. We appeal for more linguistically interpretable and grounded representations, careful justification of design decisions and evaluation of the effectiveness of different approaches.
      </p>

      <div class="embed-responsive embed-responsive-16by9 collapse" id="collapseVideo-2zcD2zylw8U">
        <p>
          <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/2zcD2zylw8U" allowfullscreen=""></iframe>
        </p>
      </div>
    </p>
  </div>
</div>

<div class="card mb-3">
  <div class="card-header">
    Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments – Extended Abstract 
  </div>
  <div class="card-body">
    <p class="card-text">
      <p>Authors: Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, Stefan Lee</p>
      <p>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=BRjplxPwk1" role="button">
          OpenReview
        </a>
        -->
        <a class="btn btn-primary" href="/assets/pdfs/beyond_the_nav_graph_vision_and_language_navigation_in_continuous_environments_extended_abstract.pdf" role="button">
          Paper
        </a>
        <a class="btn btn-primary" data-toggle="collapse" href="#collapseVideo-WudI1Up1pSc" role="button" aria-expanded="false" aria-controls="collapseVideo-WudI1Up1pSc">
          Video
        </a>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=BRjplxPwk1" role="button">
          Slides
        </a>
        -->
      </p>
      <p>
      We develop a language-guided navigation task set in a continuous 3D environment where agents must execute low-level actions to follow natural language navigation directions. By being situated in continuous environments, this setting lifts a number of assumptions implicit in prior work that represents environments as a sparse graph of panoramas with edges corresponding to navigability. Specifically, our setting drops the presumptions of known environment topologies, short-range oracle navigation, and perfect agent localization. To contextualize this new task, we develop models that mirror many of the advances made in prior settings. We find significantly lower performance in the continuous setting -- suggesting that performance in topological settings may be inflated by the strong implicit assumptions.
      </p>

      <div class="embed-responsive embed-responsive-16by9 collapse" id="collapseVideo-WudI1Up1pSc">
        <p>
          <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/WudI1Up1pSc" allowfullscreen=""></iframe>
        </p>
      </div>
    </p>
  </div>
</div>

<div class="card mb-3">
  <div class="card-header">
    Reinforcement Communication Learning in Different Social Network Structures
  </div>
  <div class="card-body">
    <p class="card-text">
      <p>Authors: Marina Dubova, Arsenii Kirillovich Moskvichev, Robert L. Goldstone</p>
      <p>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=K9YMQIu-eDx" role="button">
          OpenReview
        </a>
        -->
        <a class="btn btn-primary" href="/assets/pdfs/reinforcement_communication_learning_in_different_social_network_structures.pdf" role="button">
          Paper
        </a>
        <a class="btn btn-primary" data-toggle="collapse" href="#collapseVideo-vFf62zLkAj0" role="button" aria-expanded="false" aria-controls="collapseVideo-vFf62zLkAj0">
          Video
        </a>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=K9YMQIu-eDx" role="button">
          Slides
        </a>
        -->
      </p>
      <p>
      Social network structure is one of the key determinants of human language evolution. Previous work has shown that the network of social interactions shapes decentralized learning in human groups, leading to the emergence of different kinds of communicative conventions. We examined the effects of social network organization on the properties of communication systems emerging in decentralized, multi-agent reinforcement learning communities. We found that the global connectivity of a social network drives the convergence of populations on shared and symmetric communication systems, preventing the agents from forming many local 'dialects'. Moreover, the agent's degree is inversely related to the consistency of its use of communicative conventions. These results show the importance of the basic properties of social network structure on reinforcement communication learning and suggest a new interpretation of findings on human convergence on word conventions.
      </p>

      <div class="embed-responsive embed-responsive-16by9 collapse" id="collapseVideo-vFf62zLkAj0">
        <p>
          <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/vFf62zLkAj0" allowfullscreen=""></iframe>
        </p>
      </div>
    </p>
  </div>
</div>

<div class="card mb-3">
  <div class="card-header">
    Pow-Wow: A Dataset and Study on Collaborative Communication in Pommerman 
  </div>
  <div class="card-body">
    <p class="card-text">
      <p>Authors: Takuma Yoneda, Matthew Walter, Jason Naradowsky</p>
      <p>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=IjAiHBsG3Wi" role="button">
          OpenReview
        </a>
        -->
        <a class="btn btn-primary" href="/assets/pdfs/pow_wow_a_dataset_and_study_on_collaborative_communication_in_pommerman.pdf" role="button">
          Paper
        </a>
        <a class="btn btn-primary" data-toggle="collapse" href="#collapseVideo-vdKLVUvyMN0" role="button" aria-expanded="false" aria-controls="collapseVideo-vdKLVUvyMN0">
          Video
        </a>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=IjAiHBsG3Wi" role="button">
          Slides
        </a>
        -->
      </p>
      <p>
      In multi-agent learning, agents must coordinate with each other in order to succeed.  For humans, this coordination is typically accomplished through the use of language.  In this work we perform a controlled study of human language use in a competitive team-based game, and search for useful lessons for structuring communication protocol between autonomous agents. We construct Pow-Wow, a new dataset for studying situated goal-directed human communication.  Using the Pommerman game environment, we enlisted teams of humans to play against teams of AI agents, recording their observations, actions, and communications. We analyze the types of communications which result in effective game strategies, annotate them accordingly, and present corpus-level statistical analysis of how trends in communications affect game outcomes.  Based on this analysis, we design a communication policy for learning agents, and show that agents which utilize communication achieve higher win-rates against baseline systems than those which do not.
      </p>

      <div class="embed-responsive embed-responsive-16by9 collapse" id="collapseVideo-vdKLVUvyMN0">
        <p>
          <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/vdKLVUvyMN0" allowfullscreen=""></iframe>
        </p>
      </div>
    </p>
  </div>
</div>

<div class="card mb-3">
  <div class="card-header">
    Pre-trained Word Embeddings for Goal-conditional Transfer Learning in Reinforcement Learning 
  </div>
  <div class="card-body">
    <p class="card-text">
      <p>Authors: Matthias Hutsebaut-Buysse, Kevin Mets, Steven Latré</p>
      <p>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=PkqYy3iyIg" role="button">
          OpenReview
        </a>
        -->
        <a class="btn btn-primary" href="/assets/pdfs/pre_trained_word_embeddings_for_goal_conditional_transfer_learning_in_reinforcement_learning.pdf" role="button">
          Paper
        </a>
        <a class="btn btn-primary" data-toggle="collapse" href="#collapseVideo-d7esiU818_I" role="button" aria-expanded="false" aria-controls="collapseVideo-d7esiU818_I">
          Video
        </a>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=PkqYy3iyIg" role="button">
          Slides
        </a>
        -->
      </p>
      <p>
      Reinforcement learning (RL) algorithms typically start tabula rasa, without any prior knowledge of the environment, and without any prior skills. This however often leads to low sample efficiency, requiring a large amount of interaction with the environment. This is especially true in a lifelong learning setting, in which the agent needs to continually extend its capabilities. In this paper, we examine how a pre-trained task-independent language model  can make  a  goal-conditional  RL agent more sample efficient.  We do this by facilitating transfer learning between different related tasks. We experimentally demonstrate our approach on a set of object navigation tasks.
      </p>

      <div class="embed-responsive embed-responsive-16by9 collapse" id="collapseVideo-d7esiU818_I">
        <p>
          <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/d7esiU818_I" allowfullscreen=""></iframe>
        </p>
      </div>
    </p>
  </div>
</div>

<div class="card mb-3">
  <div class="card-header">
    WordCraft: An Environment for Benchmarking Commonsense Agents
  </div>
  <div class="card-body">
    <p class="card-text">
      <p>Authors: Minqi Jiang, Jelena Luketina, Nantas Nardelli, Pasquale Minervini, Philip Torr, Shimon Whiteson, Tim Rocktäschel</p>
      <p>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=A0wHsDMUXAE" role="button">
          OpenReview
        </a>
        -->
        <a class="btn btn-primary" href="/assets/pdfs/wordcraft_an_environment_for_benchmarking_commonsense_agents.pdf" role="button">
          Paper
        </a>
        <a class="btn btn-primary" data-toggle="collapse" href="#collapseVideo-zta8a4kQMlY" role="button" aria-expanded="false" aria-controls="collapseVideo-zta8a4kQMlY">
          Video
        </a>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=A0wHsDMUXAE" role="button">
          Slides
        </a>
        -->
      </p>
      <p>
      The ability to quickly solve a wide range of real-world tasks requires a commonsense understanding of the world. Yet, how to best extract such knowledge from natural language corpora and integrate it with reinforcement learning (RL) agents remains an open challenge. This is partly due to the lack of lightweight simulation environments that sufficiently reflect the semantics of the real world and provide knowledge sources grounded with respect to observations in an RL environment. To enable research on benchmarking agents with commonsense knowledge, we propose WordCraft, an RL environment based on LittleAlchemy2. This environment is small and fast to run, but built upon entities and relations inspired by real-world semantics. We evaluate several representation learning methods on this benchmarks and propose a new method for integrating knowledge graphs within an RL agent.
      </p>

      <div class="embed-responsive embed-responsive-16by9 collapse" id="collapseVideo-zta8a4kQMlY">
        <p>
          <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/zta8a4kQMlY" allowfullscreen=""></iframe>
        </p>
      </div>
    </p>
  </div>
</div>

<div class="card mb-3">
  <div class="card-header">
    Emergence of Multilingualism in Population based Referential Games
  </div>
  <div class="card-body">
    <p class="card-text">
      <p>Authors: Shresth Verma</p>
      <p>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=AGcx71S1TTE" role="button">
          OpenReview
        </a>
        -->
        <a class="btn btn-primary" href="/assets/pdfs/emergence_of_multilingualism_in_population_based_referential_games.pdf" role="button">
          Paper
        </a>
        <a class="btn btn-primary" data-toggle="collapse" href="#collapseVideo-X8WnvB6PHHs" role="button" aria-expanded="false" aria-controls="collapseVideo-X8WnvB6PHHs">
          Video
        </a>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=AGcx71S1TTE" role="button">
          Slides
        </a>
        -->
      </p>
      <p>
      The ability of agents to learn to communicate by interaction has been studied through emergent communication tasks. Inspired by previous work in this domain, we extend the referential game setup to a population of spatially distributed agents. In such a setting, our experiments reveal that multiple languages can emerge in the population and some agents develop multilingual traits. 
Further, an action-advising framework is proposed for improving sample efficiency in the learning process.
      </p>

      <div class="embed-responsive embed-responsive-16by9 collapse" id="collapseVideo-X8WnvB6PHHs">
        <p>
          <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/X8WnvB6PHHs" allowfullscreen=""></iframe>
        </p>
      </div>
    </p>
  </div>
</div>

<div class="card mb-3">
  <div class="card-header">
    Emergence of compositional language in communication through noisy channel
  </div>
  <div class="card-body">
    <p class="card-text">
      <p>Authors: Łukasz Kuciński, Paweł Kołodziej, Piotr Miłoś</p>
      <p>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=ZbXlSL_xwtA" role="button">
          OpenReview
        </a>
        -->
        <a class="btn btn-primary" href="/assets/pdfs/emergence_of_compositional_language_in_communication_through_noisy_channel.pdf" role="button">
          Paper
        </a>
        <a class="btn btn-primary" data-toggle="collapse" href="#collapseVideo-GtGcX_s90Ag" role="button" aria-expanded="false" aria-controls="collapseVideo-GtGcX_s90Ag">
          Video
        </a>
        <!--
        <a class="btn btn-primary" href="https://openreview.net/forum?id=ZbXlSL_xwtA" role="button">
          Slides
        </a>
        -->
      </p>
      <p>
      In this paper, we investigate how communication through a noisy channel can lead to the emergence of compositional language. Our approach is , allows for different inductive biases on the agents’ architecture, and trains without periodical resets of the networks’ weights. This relaxes some of the assumptions in recently developed methods. The impact on the structure of the resulting language is shown in the context of signaling games. We also develop a new metric for measuring degree of compositionality.
      </p>

      <div class="embed-responsive embed-responsive-16by9 collapse" id="collapseVideo-GtGcX_s90Ag">
        <p>
          <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/GtGcX_s90Ag" allowfullscreen=""></iframe>
        </p>
      </div>
    </p>
  </div>
</div>


  
  </section>
</article>

    </div>
    <footer class="site-footer">
    <p class="text">
        Copyright 2020</p>
            <div class="footer-icons">
                <ul>
                <!-- Social icons from Font Awesome, if enabled -->
                













































                </ul>
            </div>
</footer>



  </body>
</html>

---
layout: page
---

# **Language in Reinforcement Learning**

### ICML 2020 Workshop, 17 or 18 July 2020

### _The workshop will take place virtually due to COVID-19 pandemic._ 

<figure class="figure">
  <img src="/assets/img/machinereading.jpg" class="figure-img img-fluid rounded" alt="Machine reading.">
</figure>

Language is one of the most impressive human accomplishments and is believed to be the core to our ability to learn, teach, reason and interact with others ([Gopnik and Meltzoff, 1987](http://ilabs.washington.edu/meltzoff/pdf/87Gopnik_Meltzoff_ChildDev.pdf); [Spelke and Kinzler, 2007](http://inst.cs.berkeley.edu/~cs182/sp08/readings/SpelkeKinzler07.pdf); [Shusterman et al., 2011](https://www.ncbi.nlm.nih.gov/pubmed/21665199)). It is hard to imagine teaching a child any complex task or skill without, at some point, relying on language to communicate. Written language has also given humans the ability to store information and insights about the world and pass it across generations and continents. Yet, current state-of-the-art reinforcement learning agents are unable to use or understand human language. 

Practically speaking, the ability to integrate and learn from language, in addition to rewards and demonstrations, has the potential to improve the generalization, scope and sample efficiency of agents. For example, agents that are capable of transferring domain knowledge from textual corpora might be able to much more efficiently explore in a given environment or to perform zero or few shot learning in novel environments. Furthermore, many real-world tasks, including personal assistants and general household robots, require agents to process language by design, whether to enable interaction with humans, or simply use existing interfaces. 

To support this emerging field of research, we are interested in fostering the discussion around:
* methods that can effectively link language to actions and observations in the environment 
([Branavan et al., 2009](https://people.csail.mit.edu/regina/my_papers/RL.pdf); [Tellex et al., 2011](https://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/viewFile/3623/4113); [Chen et al., 2011](http://www.cs.utexas.edu/users/ml/papers/chen.aaai11.pdf); [Bisk et al., 2016](https://yonatanbisk.com/papers/2016-NAACL.pdf); 
[Mei et al., 2016](https://arxiv.org/abs/1506.04089); [Hermann et al., 2017](https://arxiv.org/abs/1706.06551)) 
* research into language roles beyond encoding goal states, such as structuring hierarchical policies ([Andreas et al., 2017](https://arxiv.org/abs/1611.01796); [Jiang et al., 2019](https://arxiv.org/abs/1906.07343)), 
communicating domain knowledge ([Narasimhan et al., 2018](https://arxiv.org/abs/1708.00133); [Zhong et al., 2020](https://arxiv.org/abs/1910.08210))
 or reward shaping ([Bahdanau et al., 2018](https://arxiv.org/abs/1806.01946); [Wang et al., 2019](https://arxiv.org/abs/1811.10092)) 
* methods that can help identify and incorporate textual information relating to the task, especially when the language is natural and unstructured ([Branavan et al., 2012](https://arxiv.org/abs/1401.5390))
* novel environments enabling such research and approaching complexity of real-world problem settings 
([Das et al., 2018](https://arxiv.org/abs/1711.11543); [Cote et al., 2018](https://arxiv.org/abs/1806.11532); [Chevalier-Boisvertet al., 2019](https://arxiv.org/abs/1810.08272); [Savva et al., 2019](https://arxiv.org/abs/1904.01201)).

The aim of the first workshop on **La**nguage in **Re**inforcement **L**earning is to steer discussion and research of these problems by bringing together researchers from several communities, including reinforcement learning, robotics, NLP, computer vision and developmental psychology. Through this workshop, we look to identify the main challenges, exchange ideas among and lessons learned from the different research threads, as well as establish requirements for evaluation benchmarks for approaches that integrate language with sequential decision making.






